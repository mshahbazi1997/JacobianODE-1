{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58b2916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ec780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from JacobianODE.jacobians.jacobian_utils import load_config\n",
    "from JacobianODE.jacobians.run_jacobians import train_jacobians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a65ce6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL THIS IN FOR YOUR OWN USE\n",
    "save_dir = \"/orcd/data/ekmiller/001/eisenaj/JacobianODE/lightning\"\n",
    "wandb_entity = \"chaotic-consciousness\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5772a0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "760d4d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the config\n",
    "overrides = [\n",
    "    \"data=dysts\",\n",
    "    \"data.flow._target_=JacobianODE.dysts_sim.flows.Lorenz\",\n",
    "    \"data.postprocessing.obs_noise=0.01\",\n",
    "    \"training.lightning.loop_closure_weight=0.001\",\n",
    "    f\"training.logger.save_dir=./output\",\n",
    "    # f\"wandb_entity={wandb_entity}\",\n",
    "]\n",
    "cfg = load_config(overrides=overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27c26fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run MLP__hidden_dim_[256, 1024, 2048, 2048]__num_layers_4__residuals_False__dropout_0.0__activation_silu__batch_size_16__direct_True__loss_func_mse__alpha_hal_0.1__l2_penalty_0__l1_penalty_0.0__obs_noise_scale_0.0000__final_obs_noise_scale_0__y0_noise_scale_0__noise_annealing_False__log_interval_100__alpha_teacher_forcing_1__teacher_forcing_annealing_True__gamma_teacher_forcing_0.999__teacher_forcing_update_interval_5__teacher_forcing_steps_1__min_alpha_teacher_forcing_0__alpha_validation_0__obs_noise_scale_validation_0.0000__loss_func_validation_mse__traj_init_steps_validation_15__inner_N_validation_20__data_type_dysts__jacobianODEint_kwargs_{'traj_init_steps': 15, 'inner_path': 'line', 'inner_N': 20, 'interp_pts': 4}__gradient_clip_val_1.0__gradient_clip_algorithm_norm__optimizer_AdamW__optimizer_kwargs_{'lr': 0.0001, 'weight_decay': 0.0001}__use_scheduler_True__min_lr_1e-06__k_scale_1__jac_penalty_0.0__jac_norm_ord_fro__loop_closure_training_True__mix_trajectories_True__loop_closure_interp_pts_20__loop_closure_int_method_Trapezoid__n_loop_pts_20__obs_noise_scale_loop_0.0__trajectory_training_True__loop_closure_weight_0.001__use_base_deriv_pt_False__obs_noise_0.0100__filter_data_False__low_pass_10__high_pass_None__normalize_False__seq_length_25__n_periods_12__pts_per_period_100__standardize_False__noise_0.0__num_ics_32 already exists, incrementing version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahdiyarshahbazi/miniconda3/envs/jacobian/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You set `strategy=ddp_notebook` but strategies from the DDP family are not supported on the MPS accelerator. Either explicitly set `accelerator='cpu'` or change the strategy.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_jacobians\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jacobian/lib/python3.11/site-packages/hydra/main.py:83\u001b[39m, in \u001b[36mmain.<locals>.main_decorator.<locals>.decorated_main\u001b[39m\u001b[34m(cfg_passthrough)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(task_function)\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorated_main\u001b[39m(cfg_passthrough: Optional[DictConfig] = \u001b[38;5;28;01mNone\u001b[39;00m) -> Any:\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m cfg_passthrough \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg_passthrough\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     85\u001b[39m         args_parser = get_args_parser()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/JacobianODE/JacobianODE/jacobians/run_jacobians.py:102\u001b[39m, in \u001b[36mtrain_jacobians\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m     97\u001b[39m log_training_info(train_dataloader, trajs, lit_model, log=log)\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# TRAIN MODEL\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity\u001b[49m\u001b[43m=\u001b[49m\u001b[43mentity\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/JacobianODE/JacobianODE/jacobians/jacobian_utils.py:530\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(cfg, lit_model, train_dataloaders, val_dataloaders, name, project, entity)\u001b[39m\n\u001b[32m    527\u001b[39m gradient_clip_val = lit_model.gradient_clip_val\n\u001b[32m    528\u001b[39m gradient_clip_algorithm = lit_model.gradient_clip_algorithm\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m trainer = \u001b[43mL\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_every_n_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_clip_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgradient_clip_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_clip_algorithm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgradient_clip_algorithm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrainer_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# accelerator='auto',\u001b[39;49;00m\n\u001b[32m    538\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# devices=1 if os.path.exists('/home/millerlab-gpu') else 'auto'\u001b[39;49;00m\n\u001b[32m    539\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrategy\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    543\u001b[39m trainer.fit(model=lit_model, train_dataloaders=train_dataloaders, val_dataloaders=val_dataloaders)\n\u001b[32m    544\u001b[39m wandb.finish()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jacobian/lib/python3.11/site-packages/lightning/pytorch/utilities/argparse.py:70\u001b[39m, in \u001b[36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m kwargs = \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlist\u001b[39m(env_variables.items()) + \u001b[38;5;28mlist\u001b[39m(kwargs.items()))\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# all args were already moved to kwargs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jacobian/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:404\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, accelerator, strategy, devices, num_nodes, precision, logger, callbacks, fast_dev_run, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, overfit_batches, val_check_interval, check_val_every_n_epoch, num_sanity_val_steps, log_every_n_steps, enable_checkpointing, enable_progress_bar, enable_model_summary, accumulate_grad_batches, gradient_clip_val, gradient_clip_algorithm, deterministic, benchmark, inference_mode, use_distributed_sampler, profiler, detect_anomaly, barebones, plugins, sync_batchnorm, reload_dataloaders_every_n_epochs, default_root_dir, model_registry)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;66;03m# init connectors\u001b[39;00m\n\u001b[32m    402\u001b[39m \u001b[38;5;28mself\u001b[39m._data_connector = _DataConnector(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m \u001b[38;5;28mself\u001b[39m._accelerator_connector = \u001b[43m_AcceleratorConnector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m    \u001b[49m\u001b[43msync_batchnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43msync_batchnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbenchmark\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbenchmark\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_distributed_sampler\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_distributed_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplugins\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplugins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28mself\u001b[39m._logger_connector = _LoggerConnector(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    417\u001b[39m \u001b[38;5;28mself\u001b[39m._callback_connector = _CallbackConnector(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jacobian/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/accelerator_connector.py:129\u001b[39m, in \u001b[36m_AcceleratorConnector.__init__\u001b[39m\u001b[34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, sync_batchnorm, benchmark, use_distributed_sampler, deterministic)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28mself\u001b[39m._layer_sync: Optional[LayerSync] = TorchSyncBatchNorm() \u001b[38;5;28;01mif\u001b[39;00m sync_batchnorm \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28mself\u001b[39m.checkpoint_io: Optional[CheckpointIO] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_config_and_set_final_flags\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplugins\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplugins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43msync_batchnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43msync_batchnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# 2. Instantiate Accelerator\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m# handle `auto`, `None` and `gpu`\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._accelerator_flag == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jacobian/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/accelerator_connector.py:217\u001b[39m, in \u001b[36m_AcceleratorConnector._check_config_and_set_final_flags\u001b[39m\u001b[34m(self, strategy, accelerator, precision, plugins, sync_batchnorm)\u001b[39m\n\u001b[32m    213\u001b[39m is_mps_accelerator = MPSAccelerator.is_available() \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m    214\u001b[39m     accelerator \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mmps\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mgpu\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(accelerator, MPSAccelerator)\n\u001b[32m    215\u001b[39m )\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_mps_accelerator \u001b[38;5;129;01mand\u001b[39;00m is_parallel_strategy:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    218\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou set `strategy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstrategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` but strategies from the DDP family are not supported on the\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    219\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m MPS accelerator. Either explicitly set `accelerator=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m\u001b[33m` or change the strategy.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    220\u001b[39m     )\n\u001b[32m    222\u001b[39m \u001b[38;5;28mself\u001b[39m._accelerator_flag = accelerator\n\u001b[32m    224\u001b[39m precision_flag = _convert_precision_to_unified_args(precision)\n",
      "\u001b[31mValueError\u001b[39m: You set `strategy=ddp_notebook` but strategies from the DDP family are not supported on the MPS accelerator. Either explicitly set `accelerator='cpu'` or change the strategy."
     ]
    }
   ],
   "source": [
    "train_jacobians(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a8533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jacobian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
